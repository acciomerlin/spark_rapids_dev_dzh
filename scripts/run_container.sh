#!/bin/bash
set -e

IMAGE_NAME="spark-rapids-dev:1.0.0"
CONTAINER_NAME="xy_rapids_dev_env"
# å®šä¹‰é¡¹ç›®ç›®å½•ï¼Œå¹¶æŒ‚è½½åˆ°å®¹å™¨å†…çš„ç›¸åŒè·¯å¾„
PROJECT_DIR=$(realpath ~/spark_rapids_dev)
CONTAINER_PROJECT_DIR="/root/spark_rapids_dev"

# ç¡®ä¿ä¸»æœºä¸Šçš„é¡¹ç›®ç›®å½•å’Œå­ç›®å½•å­˜åœ¨
mkdir -p "${PROJECT_DIR}/source"
mkdir -p "${PROJECT_DIR}/data"
mkdir -p "${PROJECT_DIR}/cache/m2_cache"
mkdir -p "${PROJECT_DIR}/cache/ccache"
mkdir -p "${PROJECT_DIR}/cache/conda_cache"

# --- æ ¸å¿ƒé€»è¾‘å˜æ›´ ---

# 1. æ£€æŸ¥å®¹å™¨æ˜¯å¦æ­£åœ¨è¿è¡Œ
if [ "$(docker ps -q -f name=^/${CONTAINER_NAME}$)" ]; then
    echo "âœ… Attaching to running container: ${CONTAINER_NAME}"
    docker exec -it "${CONTAINER_NAME}" /bin/bash
    exit 0
# 2. å¦‚æžœæ²¡æœ‰åœ¨è¿è¡Œï¼Œæ£€æŸ¥å®¹å™¨æ˜¯å¦å­˜åœ¨ï¼ˆå³å¤„äºŽåœæ­¢çŠ¶æ€ï¼‰
elif [ "$(docker ps -aq -f name=^/${CONTAINER_NAME}$)" ]; then
    echo "ðŸš€ Starting existing stopped container: ${CONTAINER_NAME}"
    # ä½¿ç”¨ docker start å¯åŠ¨å·²åœæ­¢çš„å®¹å™¨ï¼Œå¹¶å°†è¾“å‡ºé‡å®šå‘åˆ° /dev/null ä»¥ä¿æŒç•Œé¢æ¸…æ´
    docker start "${CONTAINER_NAME}" > /dev/null
    echo "âœ… Attaching to container..."
    docker exec -it "${CONTAINER_NAME}" /bin/bash
    exit 0
fi

# 3. å¦‚æžœå®¹å™¨å®Œå…¨ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–°å®¹å™¨
echo "âœ¨ Creating and starting new container: ${CONTAINER_NAME}"

docker run -it  \
    --name "${CONTAINER_NAME}" \
    --gpus all \
    -p 4040:4040 \
    -p 18081:18081 \
    -p 18082:18082 \
    --shm-size=4g \
    -v "${PROJECT_DIR}:${CONTAINER_PROJECT_DIR}" \
    -v "${PROJECT_DIR}/cache/m2_cache:/root/.m2" \
    -v "${PROJECT_DIR}/cache/ccache:/root/.ccache" \
    -v "${PROJECT_DIR}/cache/conda_cache:/root/.conda/pkgs" \
    -w "${CONTAINER_PROJECT_DIR}" \
    "${IMAGE_NAME}" \
    /bin/bash -c "echo 'export SPARK_HOME=${CONTAINER_PROJECT_DIR}/source/spark-3.5.6-bin-hadoop3' >> /root/.bashrc && echo 'export PATH=\$PATH:\$SPARK_HOME/bin:\$SPARK_HOME/sbin' >> /root/.bashrc && exec /bin/bash"